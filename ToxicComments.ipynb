{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. \n",
    "\n",
    "##### The types of toxicity are:\n",
    "\n",
    "toxic\n",
    "\n",
    "severe_toxic\n",
    "\n",
    "obscene\n",
    "\n",
    "threat\n",
    "\n",
    "insult\n",
    "\n",
    "identity_hate\n",
    "\n",
    "#### You must create a model which predicts a probability of each type of toxicity for each comment.\n",
    "\n",
    "##### File descriptions\n",
    "\n",
    "train.csv - the training set, contains comments with their binary labels\n",
    "\n",
    "test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n",
    "\n",
    "sample_submission.csv - a sample submission file in the correct format\n",
    "\n",
    "test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/felipesantos/Desktop/BootcampNY/TreeProject/Toxic Comment Classification Challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100103</th>\n",
       "      <td>17cb79c13491ecd4</td>\n",
       "      <td>It was copied directly from the website. I hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110668</th>\n",
       "      <td>4ffff3f4bac51ec2</td>\n",
       "      <td>I'm sorry you feel that way - the idea is simp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59961</th>\n",
       "      <td>a0822b1d56b54015</td>\n",
       "      <td>\"\\nMacedonia has nothing to do with the articl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58433</th>\n",
       "      <td>9c7abe1474bab505</td>\n",
       "      <td>Sarah Palin's family \\n\\nWhy (or for that matt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34904</th>\n",
       "      <td>5d3a5b5b79a2d81d</td>\n",
       "      <td>I just saw you redacting per this request. Fei...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100424</th>\n",
       "      <td>199cfaa56b34ef74</td>\n",
       "      <td>JackofOz|JackofOz]] sums up my feeling about i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93880</th>\n",
       "      <td>fb064a6df316fdf7</td>\n",
       "      <td>Mark 5 variant?\\nDoes anyone have information ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79027</th>\n",
       "      <td>d37ee2a3d55cac06</td>\n",
       "      <td>. Plus I not a friggin smart dude, so I cant u...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110610</th>\n",
       "      <td>4fb541e7858942c3</td>\n",
       "      <td>I did not make the changes you claim.  There a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92598</th>\n",
       "      <td>f7a400c0eda44b97</td>\n",
       "      <td>Well that's the thing, we can't include anythi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "100103  17cb79c13491ecd4  It was copied directly from the website. I hav...   \n",
       "110668  4ffff3f4bac51ec2  I'm sorry you feel that way - the idea is simp...   \n",
       "59961   a0822b1d56b54015  \"\\nMacedonia has nothing to do with the articl...   \n",
       "58433   9c7abe1474bab505  Sarah Palin's family \\n\\nWhy (or for that matt...   \n",
       "34904   5d3a5b5b79a2d81d  I just saw you redacting per this request. Fei...   \n",
       "100424  199cfaa56b34ef74  JackofOz|JackofOz]] sums up my feeling about i...   \n",
       "93880   fb064a6df316fdf7  Mark 5 variant?\\nDoes anyone have information ...   \n",
       "79027   d37ee2a3d55cac06  . Plus I not a friggin smart dude, so I cant u...   \n",
       "110610  4fb541e7858942c3  I did not make the changes you claim.  There a...   \n",
       "92598   f7a400c0eda44b97  Well that's the thing, we can't include anythi...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "100103      0             0        0       0       0              0  \n",
       "110668      0             0        0       0       0              0  \n",
       "59961       0             0        0       0       0              0  \n",
       "58433       0             0        0       0       0              0  \n",
       "34904       0             0        0       0       0              0  \n",
       "100424      0             0        0       0       0              0  \n",
       "93880       0             0        0       0       0              0  \n",
       "79027       1             0        0       0       0              0  \n",
       "110610      0             0        0       0       0              0  \n",
       "92598       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x111dc9358>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111da7cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame([df.toxic.mean(), \n",
    "              df.severe_toxic.mean(), \n",
    "              df.obscene.mean(), \n",
    "              df.threat.mean(),\n",
    "              df.identity_hate.mean(), \n",
    "              df.insult.mean()], \n",
    "             index=['toxic','severe_toxic','obscene','threat','identity_hate','insult'], \n",
    "            columns=['happening']).sort_values('happening', ascending=False).reset_index().plot(x='index',\n",
    "                                                                                                y='happening', \n",
    "                                                                                                kind='bar', \n",
    "                                                                                                legend=False, \n",
    "                                                                                                grid=True, \n",
    "                                                                                                figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>0c3dff4d5928a933</td>\n",
       "      <td>Who cares anymore. They attack with impunity.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                   comment_text  toxic  \\\n",
       "4618  0c3dff4d5928a933  Who cares anymore. They attack with impunity.      0   \n",
       "\n",
       "      severe_toxic  obscene  threat  insult  identity_hate  \n",
       "4618             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['new_label'] = [str(a)+str(b)+str(c)+str(d)+str(e)+str(f)  for a,b,c,d,e,f in zip \n",
    "#                   (df.toxic,df.severe_toxic,df.obscene,df.threat,df.insult,df.identity_hate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10c77b908>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d76630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = df.comment_text.str.len()\n",
    "lens.hist(bins = np.arange(0,5000,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Missing\n",
    "df['comment_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.comment_text = [clean_text(text) for text in df.comment_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passos = 159571\n",
    "# aux = pd.DataFrame(list(range(passos)), columns=['i'])\n",
    "# aux['i'] =aux['i']*df.shape[0]/passos\n",
    "# aux['i'] = [int(i) for i in aux['i']]\n",
    "# j = list(aux.i)[1:] \n",
    "# j.append(train.shape[0])\n",
    "# aux['j'] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aux_= pd.DataFrame([])\n",
    "# aux_train = pd.DataFrame([])\n",
    "# for w in range(aux.shape[0]):\n",
    "#     aux_ = df[['comment_text']].loc[range(aux.loc[w].i,aux.loc[w].j)].copy()\n",
    "#     try:\n",
    "#         aux_['comment_text_stemmer'] = [' '.join([stemmer.stem(word) for word in word_.split()]) for \n",
    "#                                         word_ in aux_['comment_text']]\n",
    "#     except :\n",
    "#         aux_['comment_text_stemmer'] =  aux_['comment_text']\n",
    "    \n",
    "#     aux_train = pd.concat([aux_train,aux_])\n",
    "#     print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aux_train.to_csv('/Users/felipesantos/Desktop/BootcampNY/TreeProject/comment_text_stemmer_2.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_ = pd.read_csv('/Users/felipesantos/Desktop/BootcampNY/TreeProject/comment_text_stemmer_2.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_text_stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explan whi the edit made under my usernam hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>d aww he matches this background colour i am s...</td>\n",
       "      <td>d aww he match thi background colour i am seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>hey man i am realli not tri to edit war it jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>more i can not make any real suggestions on im...</td>\n",
       "      <td>more i can not make ani real suggest on improv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>you sir are my hero ani chanc you rememb what ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       comment_text  \\\n",
       "0           0  explanation why the edits made under my userna...   \n",
       "1           1  d aww he matches this background colour i am s...   \n",
       "2           2  hey man i am really not trying to edit war it ...   \n",
       "3           3  more i can not make any real suggestions on im...   \n",
       "4           4  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                comment_text_stemmer  \n",
       "0  explan whi the edit made under my usernam hard...  \n",
       "1  d aww he match thi background colour i am seem...  \n",
       "2  hey man i am realli not tri to edit war it jus...  \n",
       "3  more i can not make ani real suggest on improv...  \n",
       "4  you sir are my hero ani chanc you rememb what ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_.drop_duplicates(subset=['comment_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['comment_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, aux_, on='comment_text',how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86942</th>\n",
       "      <td>e95c7d0d34ea25f0</td>\n",
       "      <td>indians or brittish was the division made up o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>indian or brittish wa the divis made up of bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>1a8c9b66bddf46c5</td>\n",
       "      <td>glad you have had pleasure from st dildo brian...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>glad you have had pleasur from st dildo brian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79852</th>\n",
       "      <td>d648ee5f05a77bb4</td>\n",
       "      <td>hey dr grossan good to see you again i sure ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey dr grossan good to see you again i sure ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23833</th>\n",
       "      <td>3f004f1df3b0eb0e</td>\n",
       "      <td>article collaboration proposal at wikiproject ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>articl collabor propos at wikiproject space co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47401</th>\n",
       "      <td>7eec99fec3c25a0f</td>\n",
       "      <td>listcruft can you nominate this article for de...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>listcruft can you nomin thi articl for delet h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "86942  e95c7d0d34ea25f0  indians or brittish was the division made up o...   \n",
       "10020  1a8c9b66bddf46c5  glad you have had pleasure from st dildo brian...   \n",
       "79852  d648ee5f05a77bb4  hey dr grossan good to see you again i sure ha...   \n",
       "23833  3f004f1df3b0eb0e  article collaboration proposal at wikiproject ...   \n",
       "47401  7eec99fec3c25a0f  listcruft can you nominate this article for de...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "86942      0             0        0       0       0              0   \n",
       "10020      0             0        0       0       0              0   \n",
       "79852      0             0        0       0       0              0   \n",
       "23833      0             0        0       0       0              0   \n",
       "47401      0             0        0       0       0              0   \n",
       "\n",
       "                                    comment_text_stemmer  \n",
       "86942  indian or brittish wa the divis made up of bri...  \n",
       "10020  glad you have had pleasur from st dildo brian ...  \n",
       "79852  hey dr grossan good to see you again i sure ha...  \n",
       "23833  articl collabor propos at wikiproject space co...  \n",
       "47401  listcruft can you nomin thi articl for delet h...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import itertools\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106372,)\n",
      "(52393,)\n"
     ]
    }
   ],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.comment_text_stemmer\n",
    "X_test = test.comment_text_stemmer\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>happening</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>0.095676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.052858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>insult</td>\n",
       "      <td>0.049280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>0.009977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.008768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.002948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  happening\n",
       "0          toxic   0.095676\n",
       "1        obscene   0.052858\n",
       "2         insult   0.049280\n",
       "3   severe_toxic   0.009977\n",
       "4  identity_hate   0.008768\n",
       "5         threat   0.002948"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([df.toxic.mean(), \n",
    "              df.severe_toxic.mean(), \n",
    "              df.obscene.mean(), \n",
    "              df.threat.mean(),\n",
    "              df.identity_hate.mean(), \n",
    "              df.insult.mean()], \n",
    "             index=['toxic','severe_toxic','obscene','threat','identity_hate','insult'], \n",
    "            columns=['happening']).sort_values('happening', ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.svm import SVC\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9486190903364954\n",
      "[[47119   192]\n",
      " [ 2500  2582]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9905330864810185\n",
      "[[51790    74]\n",
      " [  422   107]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9746149294753116\n",
      "[[49460   131]\n",
      " [ 1199  1603]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9970988490828927\n",
      "[[52230     7]\n",
      " [  145    11]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.9665604183765006\n",
      "[[49607   207]\n",
      " [ 1545  1034]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9913538068062527\n",
      "[[51863    32]\n",
      " [  421    77]]\n",
      "CPU times: user 1h 20min 37s, sys: 39.3 s, total: 1h 21min 17s\n",
      "Wall time: 1h 21min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_AdaBoostClassifier = {}\n",
    "dict_AdaBoostClassifier['toxic'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['severe_toxic'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['obscene'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['threat'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['insult'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['identity_hate'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(loss='exponential',\n",
    "                                                                                n_estimators= dict_AdaBoostClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_AdaBoostClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_AdaBoostClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.950260530986964\n",
      "[[47083   228]\n",
      " [ 2378  2704]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9905521730002099\n",
      "[[51774    90]\n",
      " [  405   124]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9752638711278224\n",
      "[[49458   133]\n",
      " [ 1163  1639]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9971179356020843\n",
      "[[52228     9]\n",
      " [  142    14]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.9676292634512244\n",
      "[[49571   243]\n",
      " [ 1453  1126]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9914683259214017\n",
      "[[51852    43]\n",
      " [  404    94]]\n",
      "CPU times: user 2h 8min 53s, sys: 1min, total: 2h 9min 54s\n",
      "Wall time: 2h 10min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_AdaBoostClassifier = {}\n",
    "dict_AdaBoostClassifier['toxic'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['severe_toxic'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['obscene'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['threat'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['insult'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['identity_hate'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(loss='exponential',\n",
    "                                                                                n_estimators= dict_AdaBoostClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_AdaBoostClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_AdaBoostClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9510430782738152\n",
      "[[47078   233]\n",
      " [ 2332  2750]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9905330864810185\n",
      "[[51768    96]\n",
      " [  400   129]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9761991105682056\n",
      "[[49447   144]\n",
      " [ 1103  1699]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9970797625637012\n",
      "[[52223    14]\n",
      " [  139    17]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.9682972916229268\n",
      "[[49564   250]\n",
      " [ 1411  1168]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9915828450365507\n",
      "[[51850    45]\n",
      " [  396   102]]\n"
     ]
    }
   ],
   "source": [
    "dict_AdaBoostClassifier = {}\n",
    "dict_AdaBoostClassifier['toxic'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['severe_toxic'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['obscene'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['threat'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['insult'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['identity_hate'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(loss='exponential',\n",
    "                                                                                n_estimators= dict_AdaBoostClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_AdaBoostClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_AdaBoostClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9543832191323268\n",
      "[[46811   500]\n",
      " [ 1890  3192]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9898077987517416\n",
      "[[51768    96]\n",
      " [  438    91]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9783558872368446\n",
      "[[49205   386]\n",
      " [  748  2054]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9969079838909778\n",
      "[[52210    27]\n",
      " [  135    21]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.9684308972572672\n",
      "[[49288   526]\n",
      " [ 1128  1451]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9904758269234439\n",
      "[[51868    27]\n",
      " [  472    26]]\n",
      "CPU times: user 1h 34min 15s, sys: 28.1 s, total: 1h 34min 43s\n",
      "Wall time: 1h 35min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_GradientBoostingClassifier = {}\n",
    "dict_GradientBoostingClassifier['toxic'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['severe_toxic'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['obscene'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['threat'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['insult'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['identity_hate'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(n_estimators= dict_GradientBoostingClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_GradientBoostingClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_GradientBoostingClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9532571145000286\n",
      "[[46741   570]\n",
      " [ 1879  3203]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9898077987517416\n",
      "[[51769    95]\n",
      " [  439    90]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9782604546408872\n",
      "[[49198   393]\n",
      " [  746  2056]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9967743782566373\n",
      "[[52209    28]\n",
      " [  141    15]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.9674574847785009\n",
      "[[49253   561]\n",
      " [ 1144  1435]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9904567404042525\n",
      "[[51866    29]\n",
      " [  471    27]]\n",
      "CPU times: user 2h 32min 27s, sys: 50.4 s, total: 2h 33min 17s\n",
      "Wall time: 2h 34min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_GradientBoostingClassifier = {}\n",
    "dict_GradientBoostingClassifier['toxic'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['severe_toxic'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['obscene'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['threat'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['insult'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['identity_hate'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(n_estimators= dict_GradientBoostingClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_GradientBoostingClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_GradientBoostingClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9537533639990075\n",
      "[[46713   598]\n",
      " [ 1825  3257]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9897696257133587\n",
      "[[51767    97]\n",
      " [  439    90]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9784131467944192\n",
      "[[49213   378]\n",
      " [  753  2049]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9967743782566373\n",
      "[[52209    28]\n",
      " [  141    15]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.9679919073158628\n",
      "[[49251   563]\n",
      " [ 1114  1465]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9904567404042525\n",
      "[[51867    28]\n",
      " [  472    26]]\n",
      "CPU times: user 3h 7min 57s, sys: 1min 4s, total: 3h 9min 1s\n",
      "Wall time: 3h 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_GradientBoostingClassifier = {}\n",
    "dict_GradientBoostingClassifier['toxic'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['severe_toxic'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['obscene'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['threat'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['insult'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_GradientBoostingClassifier['identity_hate'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(n_estimators= dict_GradientBoostingClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_GradientBoostingClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_GradientBoostingClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9569789857423702\n",
      "[[46975   336]\n",
      " [ 1918  3164]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9902277021739545\n",
      "[[51756   108]\n",
      " [  404   125]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9805508369438666\n",
      "[[49339   252]\n",
      " [  767  2035]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9970988490828927\n",
      "[[52207    30]\n",
      " [  122    34]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.971083923424885\n",
      "[[49405   409]\n",
      " [ 1106  1473]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9919073158628061\n",
      "[[51829    66]\n",
      " [  358   140]]\n",
      "CPU times: user 31min 12s, sys: 10.8 s, total: 31min 22s\n",
      "Wall time: 31min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_XGBClassifier = {}\n",
    "dict_XGBClassifier['toxic'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['severe_toxic'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['obscene'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['threat'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['insult'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['identity_hate'] = {'n_estimators': 500,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(xgb.XGBClassifier(n_estimators= dict_XGBClassifier[category]['n_estimators'],\n",
    "                                                                       max_depth= dict_XGBClassifier[category]['max_depth'],\n",
    "                                                                       learning_rate= dict_XGBClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.957856965625179\n",
      "[[46926   385]\n",
      " [ 1823  3259]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9902277021739545\n",
      "[[51745   119]\n",
      " [  393   136]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9802645391559941\n",
      "[[49320   271]\n",
      " [  763  2039]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9970797625637012\n",
      "[[52204    33]\n",
      " [  120    36]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.9714847403279064\n",
      "[[49388   426]\n",
      " [ 1068  1511]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9917355371900827\n",
      "[[51822    73]\n",
      " [  360   138]]\n",
      "CPU times: user 48min 31s, sys: 15.4 s, total: 48min 47s\n",
      "Wall time: 49min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_XGBClassifier = {}\n",
    "dict_XGBClassifier['toxic'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['severe_toxic'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['obscene'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['threat'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['insult'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['identity_hate'] = {'n_estimators': 800,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(xgb.XGBClassifier(n_estimators= dict_XGBClassifier[category]['n_estimators'],\n",
    "                                                                       max_depth= dict_XGBClassifier[category]['max_depth'],\n",
    "                                                                       learning_rate= dict_XGBClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9583723016433493\n",
      "[[46909   402]\n",
      " [ 1779  3303]]\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9901895291355716\n",
      "[[51738   126]\n",
      " [  388   141]]\n",
      "... Processing obscene\n",
      "Test accuracy is 0.9802454526368026\n",
      "[[49310   281]\n",
      " [  754  2048]]\n",
      "... Processing threat\n",
      "Test accuracy is 0.9970606760445098\n",
      "[[52203    34]\n",
      " [  120    36]]\n",
      "... Processing insult\n",
      "Test accuracy is 0.9713702212127574\n",
      "[[49385   429]\n",
      " [ 1071  1508]]\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9917546237092741\n",
      "[[51821    74]\n",
      " [  358   140]]\n",
      "CPU times: user 59min 41s, sys: 18.2 s, total: 59min 59s\n",
      "Wall time: 1h 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_XGBClassifier = {}\n",
    "dict_XGBClassifier['toxic'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['severe_toxic'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['obscene'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['threat'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['insult'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "dict_XGBClassifier['identity_hate'] = {'n_estimators': 1000,'max_depth': 3, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(xgb.XGBClassifier(n_estimators= dict_XGBClassifier[category]['n_estimators'],\n",
    "                                                                       max_depth= dict_XGBClassifier[category]['max_depth'],\n",
    "                                                                       learning_rate= dict_XGBClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.957704273471647\n",
      "[[46889   422]\n",
      " [ 1794  3288]]\n",
      "CPU times: user 9min 20s, sys: 3.33 s, total: 9min 23s\n",
      "Wall time: 9min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_XGBClassifier = {}\n",
    "dict_XGBClassifier['toxic'] = {'n_estimators': 500,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(xgb.XGBClassifier(n_estimators= dict_XGBClassifier[category]['n_estimators'],\n",
    "                                                                       max_depth= dict_XGBClassifier[category]['max_depth'],\n",
    "                                                                       learning_rate= dict_XGBClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9581050903746684\n",
      "[[46861   450]\n",
      " [ 1745  3337]]\n",
      "CPU times: user 14min 37s, sys: 4.15 s, total: 14min 41s\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_XGBClassifier = {}\n",
    "dict_XGBClassifier['toxic'] = {'n_estimators': 800,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(xgb.XGBClassifier(n_estimators= dict_XGBClassifier[category]['n_estimators'],\n",
    "                                                                       max_depth= dict_XGBClassifier[category]['max_depth'],\n",
    "                                                                       learning_rate= dict_XGBClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9582386960090088\n",
      "[[46837   474]\n",
      " [ 1714  3368]]\n",
      "CPU times: user 18min 4s, sys: 3.89 s, total: 18min 8s\n",
      "Wall time: 18min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_XGBClassifier = {}\n",
    "dict_XGBClassifier['toxic'] = {'n_estimators': 1000,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(xgb.XGBClassifier(n_estimators= dict_XGBClassifier[category]['n_estimators'],\n",
    "                                                                       max_depth= dict_XGBClassifier[category]['max_depth'],\n",
    "                                                                       learning_rate= dict_XGBClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9532952875384116\n",
      "[[46647   664]\n",
      " [ 1783  3299]]\n",
      "CPU times: user 27min 10s, sys: 6.58 s, total: 27min 16s\n",
      "Wall time: 27min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_GradientBoostingClassifier = {}\n",
    "dict_GradientBoostingClassifier['toxic'] = {'n_estimators': 500,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(n_estimators= dict_GradientBoostingClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_GradientBoostingClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_GradientBoostingClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.95199740423339\n",
      "[[46571   740]\n",
      " [ 1775  3307]]\n",
      "CPU times: user 41min 19s, sys: 11.9 s, total: 41min 31s\n",
      "Wall time: 42min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_GradientBoostingClassifier = {}\n",
    "dict_GradientBoostingClassifier['toxic'] = {'n_estimators': 800,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(n_estimators= dict_GradientBoostingClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_GradientBoostingClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_GradientBoostingClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9525890863283263\n",
      "[[46590   721]\n",
      " [ 1763  3319]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'severe_toxic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'severe_toxic'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_GradientBoostingClassifier = {}\n",
    "dict_GradientBoostingClassifier['toxic'] = {'n_estimators': 1000,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(n_estimators= dict_GradientBoostingClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_GradientBoostingClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_GradientBoostingClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106372,)\n",
      "(52393,)\n"
     ]
    }
   ],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "categories = ['toxic']\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.comment_text_stemmer\n",
    "X_test = test.comment_text_stemmer\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9509476456778577\n",
      "[[47088   223]\n",
      " [ 2347  2735]]\n",
      "CPU times: user 41min 9s, sys: 15.1 s, total: 41min 25s\n",
      "Wall time: 57min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_AdaBoostClassifier = {}\n",
    "dict_AdaBoostClassifier['toxic'] = {'n_estimators': 500,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['severe_toxic'] = {'n_estimators': 500,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['obscene'] = {'n_estimators': 500,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['threat'] = {'n_estimators': 500,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['insult'] = {'n_estimators': 500,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['identity_hate'] = {'n_estimators': 500,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(loss='exponential',\n",
    "                                                                                n_estimators= dict_AdaBoostClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_AdaBoostClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_AdaBoostClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9525127402515603\n",
      "[[47037   274]\n",
      " [ 2214  2868]]\n",
      "CPU times: user 1h 5min 23s, sys: 24.1 s, total: 1h 5min 47s\n",
      "Wall time: 1h 31min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_AdaBoostClassifier = {}\n",
    "dict_AdaBoostClassifier['toxic'] = {'n_estimators': 800,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['severe_toxic'] = {'n_estimators': 800,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['obscene'] = {'n_estimators': 800,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['threat'] = {'n_estimators': 800,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['insult'] = {'n_estimators': 800,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['identity_hate'] = {'n_estimators': 800,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(loss='exponential',\n",
    "                                                                                n_estimators= dict_AdaBoostClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_AdaBoostClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_AdaBoostClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9541732674212204\n",
      "[[47041   270]\n",
      " [ 2131  2951]]\n"
     ]
    }
   ],
   "source": [
    "dict_AdaBoostClassifier = {}\n",
    "dict_AdaBoostClassifier['toxic'] = {'n_estimators': 1000,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['severe_toxic'] = {'n_estimators': 1000,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['obscene'] = {'n_estimators': 1000,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['threat'] = {'n_estimators': 1000,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['insult'] = {'n_estimators': 1000,'max_depth': 5, 'learning_rate': 0.2}\n",
    "dict_AdaBoostClassifier['identity_hate'] = {'n_estimators': 1000,'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words,ngram_range=(1,1))),\n",
    "                         ('clf', OneVsRestClassifier(GradientBoostingClassifier(loss='exponential',\n",
    "                                                                                n_estimators= dict_AdaBoostClassifier[category]['n_estimators'],\n",
    "                                                                                max_depth= dict_AdaBoostClassifier[category]['max_depth'],\n",
    "                                                                                learning_rate= dict_AdaBoostClassifier[category]['learning_rate'])))])\n",
    "    print('... Processing {}'.format(category))\n",
    "    pipeline.fit(X_train, train[category])\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(confusion_matrix(test[category], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(start=7, stop=9, num=3, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106372,)\n",
      "(52393,)\n"
     ]
    }
   ],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "categories = ['toxic']\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.comment_text_stemmer\n",
    "X_test = test.comment_text_stemmer\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [4,8]\n",
    "learning_rate= [0.05, 0.8]\n",
    "n_estimators= [300, 600, 900]\n",
    "subsample= [0.5, 1.0]\n",
    "colsample_bylevel= [0.33]\n",
    "colsample_bytree= [0.5]\n",
    "reg_alpha= [0.15]\n",
    "reg_lambda= [0.15]\n",
    "\n",
    "params = list(itertools.product(max_depth, \n",
    "                                learning_rate,\n",
    "                                n_estimators,\n",
    "                                subsample, \n",
    "                                colsample_bylevel, \n",
    "                                colsample_bytree,\n",
    "                                reg_alpha,\n",
    "                                reg_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "... Processing toxic\n",
      "(4, 0.05, 300, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9473212070314737\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 10 s\n",
      "1\n",
      "... Processing toxic\n",
      "(4, 0.05, 300, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9467104384173458\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 11 s\n",
      "2\n",
      "... Processing toxic\n",
      "(4, 0.05, 600, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9531616819040711\n",
      "CPU times: user 5 s, sys: 1e+03 ns, total: 6 s\n",
      "Wall time: 9.78 s\n",
      "3\n",
      "... Processing toxic\n",
      "(4, 0.05, 600, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9525127402515603\n",
      "CPU times: user 4 s, sys: 0 ns, total: 4 s\n",
      "Wall time: 10 s\n",
      "4\n",
      "... Processing toxic\n",
      "(4, 0.05, 900, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9557956215524974\n",
      "CPU times: user 5 s, sys: 1e+03 ns, total: 6 s\n",
      "Wall time: 10 s\n",
      "5\n",
      "... Processing toxic\n",
      "(4, 0.05, 900, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9549176416696887\n",
      "CPU times: user 4 s, sys: 0 ns, total: 4 s\n",
      "Wall time: 8.82 s\n",
      "6\n",
      "... Processing toxic\n",
      "(4, 0.8, 300, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9490389937587083\n",
      "CPU times: user 6 s, sys: 1e+03 ns, total: 7 s\n",
      "Wall time: 12.6 s\n",
      "7\n",
      "... Processing toxic\n",
      "(4, 0.8, 300, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9544023056515183\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 9.06 s\n",
      "8\n",
      "... Processing toxic\n",
      "(4, 0.8, 600, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9494970702193041\n",
      "CPU times: user 5 s, sys: 1e+03 ns, total: 6 s\n",
      "Wall time: 11 s\n",
      "9\n",
      "... Processing toxic\n",
      "(4, 0.8, 600, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9549367281888802\n",
      "CPU times: user 4 s, sys: 1 s, total: 5 s\n",
      "Wall time: 10 s\n",
      "10\n",
      "... Processing toxic\n",
      "(4, 0.8, 900, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9492298589506232\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 10 s\n",
      "11\n",
      "... Processing toxic\n",
      "(4, 0.8, 900, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9550130742656462\n",
      "CPU times: user 4 s, sys: 0 ns, total: 4 s\n",
      "Wall time: 9.06 s\n",
      "12\n",
      "... Processing toxic\n",
      "(8, 0.05, 300, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9534670662111351\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 10 s\n",
      "13\n",
      "... Processing toxic\n",
      "(8, 0.05, 300, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9531044223464966\n",
      "CPU times: user 6 s, sys: 1e+03 ns, total: 7 s\n",
      "Wall time: 13.1 s\n",
      "14\n",
      "... Processing toxic\n",
      "(8, 0.05, 600, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9568072070696467\n",
      "CPU times: user 6 s, sys: 1 s, total: 7 s\n",
      "Wall time: 13.1 s\n",
      "15\n",
      "... Processing toxic\n",
      "(8, 0.05, 600, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9569026396656042\n",
      "CPU times: user 5 s, sys: 1e+03 ns, total: 6 s\n",
      "Wall time: 13.1 s\n",
      "16\n",
      "... Processing toxic\n",
      "(8, 0.05, 900, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9582386960090088\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 10 s\n",
      "17\n",
      "... Processing toxic\n",
      "(8, 0.05, 900, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9578760521443704\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 10 s\n",
      "18\n",
      "... Processing toxic\n",
      "(8, 0.8, 300, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9472066879163247\n",
      "CPU times: user 4 s, sys: 0 ns, total: 4 s\n",
      "Wall time: 10 s\n",
      "19\n",
      "... Processing toxic\n",
      "(8, 0.8, 300, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9548412955929227\n",
      "CPU times: user 5 s, sys: 1e+03 ns, total: 6 s\n",
      "Wall time: 10 s\n",
      "20\n",
      "... Processing toxic\n",
      "(8, 0.8, 600, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.947874716088027\n",
      "CPU times: user 6 s, sys: 1 s, total: 7 s\n",
      "Wall time: 15.3 s\n",
      "21\n",
      "... Processing toxic\n",
      "(8, 0.8, 600, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.955471150726242\n",
      "CPU times: user 4 s, sys: 0 ns, total: 4 s\n",
      "Wall time: 8.82 s\n",
      "22\n",
      "... Processing toxic\n",
      "(8, 0.8, 900, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9486190903364954\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 10 s\n",
      "23\n",
      "... Processing toxic\n",
      "(8, 0.8, 900, 1.0, 0.33, 0.5, 0.15, 0.15)\n",
      "0.955471150726242\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 8.82 s\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame([])\n",
    "for i in range(len(params)):\n",
    "    print(i)\n",
    "    tree_ = xgb.XGBClassifier(max_depth = params[i][0],\n",
    "                              learning_rate = params[i][1], \n",
    "                              n_estimators = params[i][2], \n",
    "                              subsample = params[i][3], \n",
    "                              colsample_bylevel = params[i][4],\n",
    "                              colsample_bytree = params[i][5],\n",
    "                              reg_alpha = params[i][6],\n",
    "                              reg_lambda = params[i][7])\n",
    "    \n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                          ('clf', OneVsRestClassifier(tree_))])\n",
    "    for category in categories:\n",
    "        print('... Processing {}'.format(category))\n",
    "        print(params[i])\n",
    "        list_aux = []\n",
    "        list_aux.append(params[i])\n",
    "        list_aux.append(category)\n",
    "        pipeline.fit(X_train, train[category])\n",
    "        prediction = pipeline.predict(X_test)\n",
    "        score = accuracy_score(test[category], prediction)\n",
    "        list_aux.append(score)\n",
    "        print(score)\n",
    "        df_result = df_result.append([pd.DataFrame([list_aux])])\n",
    "        %time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.05, 300, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.947321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.05, 300, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.946710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.05, 600, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.953162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.05, 600, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.952513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.05, 900, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.955796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.05, 900, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.954918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.8, 300, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.949039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.8, 300, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.954402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.8, 600, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.949497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.8, 600, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.954937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.8, 900, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.949230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4, 0.8, 900, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.955013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.05, 300, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.953467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.05, 300, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.953104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.05, 600, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.956807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.05, 600, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.956903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.05, 900, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.958239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.05, 900, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.957876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.8, 300, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.947207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.8, 300, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.954841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.8, 600, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.947875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.8, 600, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.955471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.8, 900, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.948619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.8, 900, 1.0, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.955471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            0      1         2\n",
       "0  (4, 0.05, 300, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.947321\n",
       "0  (4, 0.05, 300, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.946710\n",
       "0  (4, 0.05, 600, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.953162\n",
       "0  (4, 0.05, 600, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.952513\n",
       "0  (4, 0.05, 900, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.955796\n",
       "0  (4, 0.05, 900, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.954918\n",
       "0   (4, 0.8, 300, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.949039\n",
       "0   (4, 0.8, 300, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.954402\n",
       "0   (4, 0.8, 600, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.949497\n",
       "0   (4, 0.8, 600, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.954937\n",
       "0   (4, 0.8, 900, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.949230\n",
       "0   (4, 0.8, 900, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.955013\n",
       "0  (8, 0.05, 300, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.953467\n",
       "0  (8, 0.05, 300, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.953104\n",
       "0  (8, 0.05, 600, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.956807\n",
       "0  (8, 0.05, 600, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.956903\n",
       "0  (8, 0.05, 900, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.958239\n",
       "0  (8, 0.05, 900, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.957876\n",
       "0   (8, 0.8, 300, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.947207\n",
       "0   (8, 0.8, 300, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.954841\n",
       "0   (8, 0.8, 600, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.947875\n",
       "0   (8, 0.8, 600, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.955471\n",
       "0   (8, 0.8, 900, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.948619\n",
       "0   (8, 0.8, 900, 1.0, 0.33, 0.5, 0.15, 0.15)  toxic  0.955471"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(8, 0.05, 900, 0.5, 0.33, 0.5, 0.15, 0.15)</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.958239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            0      1         2\n",
       "0  (8, 0.05, 900, 0.5, 0.33, 0.5, 0.15, 0.15)  toxic  0.958239"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result[df_result[2] == df_result[2].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_1 = df_result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106372,)\n",
      "(52393,)\n"
     ]
    }
   ],
   "source": [
    "categories = ['toxic']\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.comment_text_stemmer\n",
    "X_test = test.comment_text_stemmer\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2000, 3444, 4888, 6333, 7777, 9222, 10666, 12111, 13555, 15000]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.linspace(start=2000, stop=15000, num=10, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [8]\n",
    "learning_rate= [0.05]\n",
    "n_estimators= list(np.linspace(start=2000, stop=15000, num=10, dtype=int))\n",
    "subsample= [0.5]\n",
    "colsample_bylevel= [0.33]\n",
    "colsample_bytree= [0.5]\n",
    "reg_alpha= [0.15]\n",
    "reg_lambda= [0.15]\n",
    "\n",
    "params = list(itertools.product(max_depth, \n",
    "                                learning_rate,\n",
    "                                n_estimators,\n",
    "                                subsample, \n",
    "                                colsample_bylevel, \n",
    "                                colsample_bytree,\n",
    "                                reg_alpha,\n",
    "                                reg_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "... Processing toxic\n",
      "(8, 0.05, 2000, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9786221938104013\n",
      "0.9589830702574772\n",
      "CPU times: user 4 s, sys: 1 s, total: 5 s\n",
      "Wall time: 9.06 s\n",
      "1\n",
      "... Processing toxic\n",
      "(8, 0.05, 3444, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9843003797991953\n",
      "0.9586395129120302\n",
      "CPU times: user 4 s, sys: 0 ns, total: 4 s\n",
      "Wall time: 9.78 s\n",
      "2\n",
      "... Processing toxic\n",
      "(8, 0.05, 4888, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9879573571992629\n",
      "0.9582577825282004\n",
      "CPU times: user 5 s, sys: 1e+03 ns, total: 6 s\n",
      "Wall time: 12.2 s\n",
      "3\n",
      "... Processing toxic\n",
      "(8, 0.05, 6333, 0.5, 0.33, 0.5, 0.15, 0.15)\n",
      "0.9903640055653743\n",
      "0.9579905712595194\n",
      "CPU times: user 4 s, sys: 1e+03 ns, total: 5 s\n",
      "Wall time: 9.3 s\n",
      "4\n",
      "... Processing toxic\n",
      "(8, 0.05, 7777, 0.5, 0.33, 0.5, 0.15, 0.15)\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame([])\n",
    "for i in range(len(params)):\n",
    "    print(i)\n",
    "    tree_ = xgb.XGBClassifier(max_depth = params[i][0],\n",
    "                              learning_rate = params[i][1], \n",
    "                              n_estimators = params[i][2], \n",
    "                              subsample = params[i][3], \n",
    "                              colsample_bylevel = params[i][4],\n",
    "                              colsample_bytree = params[i][5],\n",
    "                              reg_alpha = params[i][6],\n",
    "                              reg_lambda = params[i][7])\n",
    "    \n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                          ('clf', OneVsRestClassifier(tree_))])\n",
    "    for category in categories:\n",
    "        print('... Processing {}'.format(category))\n",
    "        print(params[i])\n",
    "        list_aux = []\n",
    "        list_aux.append(params[i][2])\n",
    "        list_aux.append(category)\n",
    "        pipeline.fit(X_train, train[category])\n",
    "        prediction = pipeline.predict(X_train)\n",
    "        score = accuracy_score(train[category], prediction)\n",
    "        list_aux.append(score)\n",
    "        print(score)\n",
    "        prediction = pipeline.predict(X_test)\n",
    "        score = accuracy_score(test[category], prediction)\n",
    "        list_aux.append(score)\n",
    "        print(score)\n",
    "        df_result = df_result.append([pd.DataFrame([list_aux])])\n",
    "        %time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.947524</td>\n",
       "      <td>0.944191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.951500</td>\n",
       "      <td>0.947398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.954847</td>\n",
       "      <td>0.949822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.957508</td>\n",
       "      <td>0.952036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.959435</td>\n",
       "      <td>0.953467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>350</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.960920</td>\n",
       "      <td>0.954479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.962424</td>\n",
       "      <td>0.955166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.955547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.964530</td>\n",
       "      <td>0.956330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.965527</td>\n",
       "      <td>0.956559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.966081</td>\n",
       "      <td>0.956807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>650</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.966965</td>\n",
       "      <td>0.957380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.967717</td>\n",
       "      <td>0.957628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.968479</td>\n",
       "      <td>0.957742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.969071</td>\n",
       "      <td>0.958029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>850</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.969710</td>\n",
       "      <td>0.958181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.970415</td>\n",
       "      <td>0.958239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>950</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.971083</td>\n",
       "      <td>0.958468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.971703</td>\n",
       "      <td>0.958678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1050</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.972211</td>\n",
       "      <td>0.958659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.972577</td>\n",
       "      <td>0.958430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1150</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.973010</td>\n",
       "      <td>0.958544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.973329</td>\n",
       "      <td>0.958563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1250</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.973799</td>\n",
       "      <td>0.958563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1300</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.974166</td>\n",
       "      <td>0.958430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1350</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.974542</td>\n",
       "      <td>0.958468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1400</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>0.958468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1450</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.975219</td>\n",
       "      <td>0.958506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.975717</td>\n",
       "      <td>0.958601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1         2         3\n",
       "0   100  toxic  0.947524  0.944191\n",
       "0   150  toxic  0.951500  0.947398\n",
       "0   200  toxic  0.954847  0.949822\n",
       "0   250  toxic  0.957508  0.952036\n",
       "0   300  toxic  0.959435  0.953467\n",
       "0   350  toxic  0.960920  0.954479\n",
       "0   400  toxic  0.962424  0.955166\n",
       "0   450  toxic  0.963458  0.955547\n",
       "0   500  toxic  0.964530  0.956330\n",
       "0   550  toxic  0.965527  0.956559\n",
       "0   600  toxic  0.966081  0.956807\n",
       "0   650  toxic  0.966965  0.957380\n",
       "0   700  toxic  0.967717  0.957628\n",
       "0   750  toxic  0.968479  0.957742\n",
       "0   800  toxic  0.969071  0.958029\n",
       "0   850  toxic  0.969710  0.958181\n",
       "0   900  toxic  0.970415  0.958239\n",
       "0   950  toxic  0.971083  0.958468\n",
       "0  1000  toxic  0.971703  0.958678\n",
       "0  1050  toxic  0.972211  0.958659\n",
       "0  1100  toxic  0.972577  0.958430\n",
       "0  1150  toxic  0.973010  0.958544\n",
       "0  1200  toxic  0.973329  0.958563\n",
       "0  1250  toxic  0.973799  0.958563\n",
       "0  1300  toxic  0.974166  0.958430\n",
       "0  1350  toxic  0.974542  0.958468\n",
       "0  1400  toxic  0.974928  0.958468\n",
       "0  1450  toxic  0.975219  0.958506\n",
       "0  1500  toxic  0.975717  0.958601"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_100_1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
